---
title: "Análisis de componentes principales"
author: "Andrea Fernandez"
date: '`r paste0("Última actualización: ", lubridate::now())`'
output:
  ioslides_presentation:
    css: ioslides.css
    mathjax: local
    self_contained: no
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, error = F, warning = F)
library(stringdist)
library(dplyr)
library(ggplot2)
library(tidyr)
```

# ACP

Basados en notas de http://teresa-ortiz.squarespace.com/est-multivariada

## ACP {.smaller}

El análisis de componentes principales (PCA) es una técnica que se utiliza 
con distintos objetivos:

1. Reducción de dimensionalidad.

2. Compresión de información con pérdida (_lossy_).

3. Extracción de características o (_features_).

4. Visualización de datos.

* PCA se puede definir como una proyección de los datos en un
espacio de dimensión menor (conocido como subespacio principal), tal que 
la varianza de los datos proyectados es máxima.

* PCA se puede definir como la proyección lineal que minimiza el costo medio 
de proyección, donde el costo promedio es la distancia media al cuadrado
entre los puntos y sus proyecciones.

## ACP: máxima varianza {.smaller}

Consideremos un vector de observaciones $(y^1,...,y^n)$ donde $y_i$ es de 
dimensión $d$. Nuestro objetivo es proyectar los datos en un espacio de 
dimensión $M<D$ maximizando la varianza de la proyección.

Básicamente, se calcula la varianza de los datos proyectados en un espacio de 
dimensión menor. Se deriva e iguala a cero para extraer el máximo de donde se
extrae que un punto estacionario está dado por el eigenvalor-eigenvector. 

En particular, el eigenvalor mayor será el que más varianza capture.

Al proyectar en más de una dimensión, se elige a cada paso subsecuente una
*dirección nueva* que maximiza la varianza de la proyección sujeto a que sea
ortogonal a las direcciones anteriores. Puede demostrarse que esta es la proyección
óptima.

## Aplicaciones

Existen múltiples aplicaciones del ACP. Veremos algunos ejemplos:

- Compresión de datos
- Preprocesamiento de datos
- Construcción de indicadores (ejemplo de reducción de dimensionalidad)

# Compresión de datos

## Contexto

Casos de variables latentes continuas.

Motivación: los puntos realmente están en una dimensión menor que en la que los
tenemos (e.g. ocupan más espacio del que necesitan). 

Pensemos en imágenes, en particular usamos los dígitos de la base de datos [mnist](http://yann.lecun.com/exdb/mnist/) en donde en una matriz de $64x64$
pixeles, se insertan dígitos. 

<img src="img/digit_rotate.jpg" style="width: 600px;"/>

## Aplicaciones: compresión 

Lectura de datos (imagenes)

```{r}
library(deepnet)
library(RColorBrewer)

mnist <- load.mnist("mnist")[["train"]]
ind_tres <- mnist[[3]] == 3
data_tres <- mnist[[2]][ind_tres, ]

data_tres <- mnist[[2]][ind_tres, ]
dim(data_tres)
```

## Aplicaciones: compresión {.tiny}

Visualizacion de las imagenes

```{r}
par(mfrow=c(1,5))

imageD <- function(vec, main = NULL){
  mat_digit <- matrix(vec, nrow = 28)[, 28:1]
  image(mat_digit, col = brewer.pal(5, "GnBu"), xaxt = "n", yaxt = "n", 
    bty = "n", asp = 1, main = main)
}

par(mar = rep(0.5, 4))
for(i in sample(1:nrow(data_tres), 5)){
  imageD(data_tres[i, ])
}
```

## Aplicaciones: compresión {.tiny}

Como cada eigenvector es un vector en el espcio original de $D$ dimensiones 
podemos representarlos como imágenes.

```{r, fig.width=9.4, fig.height=2.5}
data_tres <- mnist[[2]][ind_tres, ]
tres_mean <- apply(data_tres, 2, mean)

S <- cov(data_tres)
eigen_S <- eigen(S)
lambda <- eigen_S$values
u <- eigen_S$vectors
par(mar=c(1,1,1,1))
par(mfrow=c(1,5))
imageD(tres_mean)
for(i in 1:4){
  imageD(u[, i])
}
```

## Aplicaciones: compresión {.tiny}

El resto de los eigenvalores en la gráfica de abajo. Graficamos 
también la medida de distorsión $J$ asociada a la elección del número de 
componentes $M$ (dada por la suma de los eigenvalores $M+1$ a $D$).

```{r, fig.width=6, fig.height=4}
D <- length(lambda)
J <- sapply(1:D, function(i){sum(lambda[i:D])})
par(mfrow=c(1,2))
plot(lambda, type = "l")
plot(J, type = "l")
```

## Aplicaciones: compresión 

La compresión está en que reemplazamos cada vector de observaciones de dimensión 
$D$ ($y_i$) por un vector de dimensión $M$.

## Aplicaciones: compresión {.tiny}

La siguiente figura muestra la compresión para distintos valores de $M$ del
prmer dígito de la base de datos. 

```{r, fig.width=9.4, fig.height=2.5}
tres_1 <- data_tres[3, ]
par(mfrow=c(1,5))
imageD(tres_1)
for(M in c(1, 10, 50, 300)){
  u_M <- u[, 1:M]
  y_M <- tres_1 %*% u_M
  y_approx <- tres_mean + y_M %*% t(u_M)
  imageD(y_approx)
}
```

# Preprocesamiento de datos

## Aplicaciones: preprocesamiento {.smaller}

Objetivo: no es la reducción de dimensión sino la transformación del conjunto de
datos de manera que se estandaricen algunas de sus propiedades.

Esto puede ser importante para el correcto funcionamiento de algunos algoritmos
o métodos.

```{r}
head(faithful)
```

Los tiempos entre erupciones son de mucho mayor magnitud que el tiempo de la 
erupción. En muchas aplicaciones (k-medias, regresión) deberíamos estandarizar.

## Aplicaciones: preprocesamiento {.smaller}

Con PCA podemos normalizar los datos (media cero, covarianza unitaria).

Si escribimos los eigenvectores como 
$$SU=UL$$

donde $L$ es una matriz diagonal con los elementos $\lambda_i$ y $U$ es una 
matriz ortogonal cuyas columnas son los vectores $u_i$. Entonces, para cada
observación $y_i$ definimos su valor transformado 

$$z_i=L^{-1/2}U^T(y_i-\bar{y})$$
es claro que el conjunto $(z_1,...,z_N)$ tiene media cero.

## Aplicaciones: preprocesamiento {.smaller}

Veamos ahora la covarianza:

$$\frac{1}{N}\sum_{j=1}^Nz_jz_j^T=\frac{1}{N}\sum_{j=1}^NL^{-1/2}U^T(y_j-\bar{y})(y_j-\bar{y})^TUL^{-1/2}$$
$$=L^{-1/2}U^TSUL^{-1/2}=L{-1/2}LL^{-1/2}=I$$

## Ejercicio: whitening (1/5)

A la operación descrita se le conoce como _whitening_ o _sphereing_.

Aplicala a los datos de faithful y compara los datos crudos contra
los preprocesados usando gráficas.

1. Pinta los datos de faithful en ggplot con eruptions en el eje x y waiting en el y.
2. Realiza un pca con los datos utilizando la funcion *prcomp*
3. Grafica los primeros dos componentes principales con ggplot


```{r, eval = F, echo = F}
head(faithful)
par(mfrow=c(1, 2))

ggplot(faithful, aes(x = eruptions, y = waiting)) +
  geom_point()

faith_pca <- prcomp(faithful, scale. = TRUE)

faithful$pc1 <- faith_pca$x[, 1]
faithful$pc2 <- faith_pca$x[, 2]
ggplot(faithful, aes(x = pc1, y = pc2)) +
  geom_point()
```

# Construcción de indicadores

## Datos

Los datos `conapo_2010.csv` reportan el porcentaje de la población analfabeta, 
sin primaria terminada, sin drenaje, etc. En este ejercicio utilizarás esta base 
de datos para construir un índice de carencias (el ínice de marginación se 
construye a nivel localidad y no municipio).

Este índice es utilizado por el Consejo Nacional de Población en México para 
medir la marginación de distintas localidades en el territorio del país.

## ACP

  * Utiliza la función prcomp de R para realizar PCA.
  
```{r}
conapo <- read.table("datos/conapo_2010.csv", header=TRUE, quote="\"")
pca <- prcomp(conapo[which(sapply(conapo, class)=="numeric")])

names(pca)
```

## Porcentaje de varianza explicada {.tiny}

  * ¿Qué porcentaje de la varianza en los datos está explicado por la
primera componente? (Realiza una gráfica usando _screeplot_)

```{r}
par(mfrow=c(1, 1))
screeplot(pca)

summary(pca)
```

Explica el 62.15\% de la varianza.

## Gráficos útiles para interpretar: biplot {.tiny}

  * Realiza una gráfica utilizando la función biplot, si no has estudiado
PCA antes discutiremos esta gráfica en la clase.

```{r}
biplot(pca)
```

## Tabla útil para interpretar {.smaller}

```{r}
library(knitr)
pca.pesos <- function(loadings, cuantas.comp=n){
    # Recibe una matriz de loadings de un PCA.
    # Recibe cuantas.comp=numero de componentes que quieres, defaultea en todos
    # Escupe un df con signo si pesa mas que 0.5 del maximo. (signo) si pesa (0.25,0.5] del maximo y nada si pesa menos que 0.25
    n <- dim(loadings)[2]
    max.divide <- function(x){
    max <- max(abs(x))
    ifelse(abs(x) < 0.25*max, "",
    ifelse(abs(x) < 0.5*max, paste0("(", sign(x), ")"), sign(x)))
    }
    if(cuantas.comp < n){
    loadings <- loadings[, 1:cuantas.comp]
    }
    results <- cbind(rownames(loadings),as.data.frame(sapply(as.data.frame(loadings), max.divide)))
    names(results) <- c('variable', colnames(loadings))
    results
}

```

## Tabla útil para interpretar {.tiny}

```{r, echo = F}
kable(pca.pesos(pca$rotation))
```

